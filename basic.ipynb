{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by importing all necessary libraries\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the iris.csv file <a href='https://www.kaggle.com/uciml/iris'>here</a>.\n",
    "\n",
    "Just put the data file in the same directory as your Python file. The Pandas library has an easy way to load in data, <span style=\"color:red\"> read_csv()</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/iris.csv')\n",
    "    \n",
    "# It is a good idea to check and make sure the data is loaded as expected.\n",
    "    \n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset has been prepared so well, we don't need to do a lot of preprocessing. One thing we may want to do though it drop the \"ID\" column, as it is just a representation of row the example is found on.\n",
    "\n",
    "As this isn't helpful we could drop it from the dataset using the <span style=\"color:red\">drop()</span> function.\n",
    "\n",
    "We now need to define the features and labels. We can do this easily with Pandas by slicing the data table and choosing certain rows/columns with <span style=\"color:red\">iloc()</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# Pandas \".iloc\" expects row_indexer, column_indexer  \n",
    "X = data.iloc[:,:-1].values\n",
    "# Now let's tell the dataframe which column we want for the target/labels.  \n",
    "y = data['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the features and labels we want, we can split the data into training and testing sets using sklearn's handy feature <span style=\"color:red\">train_test_split()</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alternate way of selecting columns:\n",
    "#X = data.iloc['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm']\n",
    "print(X)\n",
    "list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test size specifies how much of the data you want to set aside for the testing set. \n",
    "# Random_state parameter is just a random seed we can use.\n",
    "# You can use it if you'd like to reproduce these specific results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#X_train\n",
    "#y_train\n",
    "#train_test_split(y, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the models. Let's try using two classifiers, a Support Vector Classifier and a K-Nearest Neighbors Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model = SVC()\n",
    "# KNN model requires you to specify n_neighbors,\n",
    "# the number of points the classifier will look at to determine what class a new point belongs to\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model.fit(X_train, y_train)\n",
    "KNN_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call has trained the model, so now we can predict and store the prediction in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_prediction = SVC_model.predict(X_test)\n",
    "KNN_prediction = KNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now evaluate how the classifier performed. There are multiple methods of evaluating a classifier's performance, and you can read more about there different methods below.\n",
    "\n",
    "In Scikit-Learn you just pass in the predictions against the ground truth labels which were stored in your test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score is the simplest way to evaluate\n",
    "print(accuracy_score(SVC_prediction, y_test))\n",
    "print(accuracy_score(KNN_prediction, y_test))\n",
    "# But Confusion Matrix and Classification Report give more details about performance\n",
    "print(confusion_matrix(SVC_prediction, y_test))\n",
    "print(classification_report(KNN_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here's the output we got on the metrics:\n",
    "\n",
    "SVC accuracy: 0.9333333333333333\n",
    "\n",
    "KNN accuracy: 0.9666666666666667\n",
    "\n",
    "At first glance, it seems KNN performed better. Here's the confusion matrix for SVC:\n",
    "\n",
    "[[ 7  0  0]\n",
    "\n",
    " [ 0 10  1]\n",
    "\n",
    " [ 0  1 11]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
